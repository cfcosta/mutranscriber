{"id":"mutranscriber-0cr","title":"Investigate 1.7B model accuracy issues","description":"The 1.7B model produces significantly worse transcription than the 0.6B model despite using the same codebase and preprocessing.\n\n## Root Cause Found: Missing Sinusoidal Position Embeddings\n\nInvestigation revealed that the audio encoder was **missing sinusoidal positional embeddings** that the official Python implementation adds after the conv_out projection.\n\n### Fix Applied\n\nAdded `SinusoidalPositionEmbedding` to audio_encoder.rs:\n- Computes sin/cos embeddings with max_timescale=10000\n- Applied after conv_out projection, before transformer layers\n- Added `max_source_positions` config (default: 1500)\n\n### Python Reference (from QwenLM/Qwen3-ASR)\n\n```python\nclass SinusoidsPositionEmbedding(nn.Module):\n    def __init__(self, length, channels, max_timescale=10000):\n        # Computes sin/cos positional embeddings\n        # Applied: padded_embed = padded_embed + positional_embedding\n```\n\n### Why This Affected 1.7B More\n\nThe larger 1.7B model has more transformer layers (24 vs 18) and larger dimensions, making it more dependent on positional information to track sequence positions through the deeper network.\n\n**Status:** Fix implemented, needs real-world testing to verify improvement.\n\n**Location:** src/audio_encoder.rs, src/config.rs","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T19:29:03.087573006-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T23:07:12.854851126-03:00","closed_at":"2026-02-02T23:07:12.854851126-03:00","close_reason":"Fixed: Added sinusoidal positional embeddings to audio encoder"}
{"id":"mutranscriber-2lu","title":"Compare audio encoder output with reference implementation","description":"Compare the audio encoder output statistics (mean, std, min, max) with the official Qwen3-ASR Python implementation to identify any preprocessing or encoding differences.\n\n**Approach:**\n1. Set up reference implementation (Python with transformers/qwen_asr)\n2. Process same test audio file through both implementations\n3. Compare mel spectrogram values\n4. Compare audio encoder output tensor statistics\n5. Identify any normalization or scaling differences\n\n**Files to compare:**\n- Mel spectrogram output\n- Audio encoder output (before LLM embedding)\n- Token embeddings\n\n**Location:** src/mel.rs, src/audio_encoder.rs","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.291548413-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T16:35:36.687279221-03:00","closed_at":"2026-02-02T16:35:36.687279221-03:00","close_reason":"Added diagnostic tools for comparing Rust vs Python implementations. Created: diagnostics/reference_diagnostics.py (Python reference), diagnostics/compare_diagnostics.py (comparison tool), diagnostics/README.md (usage guide). Set MUTRANSCRIBER_DIAGNOSTICS env var to dump tensor stats from Rust. Current Rust values documented for test audio."}
{"id":"mutranscriber-4b0","title":"Add tensor shape validation in model pipeline","description":"Add assertions to validate tensor shapes throughout the pipeline to catch silent failures early.\n\n**Locations needing validation:**\n- Embedding concatenation (model.rs:328) - verify pre_embed, audio_features, post_embed have same hidden_size\n- Token input preparation (model.rs:390) - verify next_input shape\n- Logits extraction (model.rs:355) - verify 3D shape before indexing\n\n**Benefits:**\n- Catch shape mismatches before they cause cryptic errors\n- Easier debugging\n- Better error messages\n\n**Location:** src/model.rs","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:31.722775716-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T22:51:06.373916677-03:00","closed_at":"2026-02-02T22:51:06.373916677-03:00","close_reason":"Added shape validation for audio features, embeddings, and logits"}
{"id":"mutranscriber-5dp","title":"Investigate task-specific tokens for ASR","description":"Whisper uses task-specific tokens like \u003c|transcribe|\u003e, \u003c|translate|\u003e, \u003c|notimestamps|\u003e. Qwen3-ASR may have similar tokens that need to be included in the prompt.\n\n**Investigation needed:**\n1. Check tokenizer vocabulary for task tokens (transcribe, asr, speech, etc.)\n2. Look for \u003c|asr_text|\u003e token ID (mentioned in paper output format)\n3. Check if there are timestamp-related tokens\n4. Review Qwen3-ASR toolkit source for prompt construction\n\n**Potentially relevant token IDs:**\n- 151643: \u003c|endoftext|\u003e\n- 151644: \u003c|im_start|\u003e\n- 151645: \u003c|im_end|\u003e\n- 151669: \u003c|audio_start|\u003e\n- 151670: \u003c|audio_end|\u003e\n- 151676: audio placeholder\n- ???: \u003c|asr_text|\u003e (from paper, ID unknown)\n\n**Location:** src/model.rs, vocab.json from HuggingFace","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.514037461-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T23:00:36.727275168-03:00","closed_at":"2026-02-02T23:00:36.727275168-03:00","close_reason":"Completed - added special_tokens module with documented token IDs"}
{"id":"mutranscriber-6ts","title":"Optimize text generation loop to reduce per-token allocations","description":"The text generation loop in model.rs creates a new 1-token tensor for each generated token (potentially 100+ iterations), causing memory allocation overhead, GPU-CPU data transfer, and kernel launch overhead for small operations.\n\n**Current code (model.rs ~574-623):**\nThe loop creates Tensor::new(\u0026[next_token], \u0026device)?.unsqueeze(0)? for each token.\n\n**Estimated impact:** 30-40% latency reduction for generation phase\n\n**Location:** src/model.rs generate() function","status":"closed","priority":1,"issue_type":"epic","owner":"me@cfcosta.com","created_at":"2026-02-03T00:38:19.648661345-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T00:46:38.249359341-03:00","closed_at":"2026-02-03T00:46:38.249359341-03:00","close_reason":"Epic complete. Applied: fast path for greedy decoding, KV cache optimization, partial sort for sampling. Research found Candle tensors are immutable - tensor reuse not feasible. Timing instrumentation added for future benchmarking."}
{"id":"mutranscriber-8nz","title":"Verify mel filterbank matches WhisperFeatureExtractor","description":"Our mel filterbank implementation may differ from WhisperFeatureExtractor.\n\n**Potential issues identified:**\n\n1. **Mel scale formula**: We use HTK formula (2595 * log10(1 + f/700)) but WhisperFeatureExtractor might use a different variant or breakpoint.\n\n2. **FFT bin calculation**: We use `((n_fft + 1) * f / sample_rate).floor()` but the standard formula is `round(f * n_fft / sample_rate)`. The +1 is suspicious.\n\n3. **Slaney normalization**: We apply area normalization but need to verify it matches exactly.\n\n**To verify:**\n- Compare our mel filterbank matrix (first few rows) against WhisperFeatureExtractor\n- Check if WhisperFeatureExtractor uses htk=True or False\n- Verify the frequency-to-bin mapping\n\n**Current behavior:**\nOutput sounds phonetically similar but wrong words (\"leapfrog\" vs \"LibriVox\")\n\n**Location:** src/mel.rs create_mel_filterbank()","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T16:58:55.079274889-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T18:18:07.993744894-03:00","closed_at":"2026-02-02T18:18:07.993744894-03:00","close_reason":"Fixed mel filterbank construction using floating-point interpolation"}
{"id":"mutranscriber-8y5","title":"Mel spectrogram computed on CPU, not GPU","description":"**Mel extraction runs entirely on CPU**\n\nThe mel spectrogram computation in mel.rs uses pure Rust FFT and mel filterbank:\n- Custom Cooley-Tukey FFT implementation\n- Per-frame processing loop\n- No GPU involvement at all\n\nFor a 30-second audio clip, this processes ~3000 frames on CPU before any GPU work begins.\n\n**Potential fix:** Use cuFFT through Candle or a CUDA FFT binding for mel computation.\n\n**Trade-off:** May not be worth it if audio encoding + text generation dominate. Profile first.\n\n**Location:** src/mel.rs (entire file)","status":"open","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T01:18:02.625662937-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T01:18:02.625662937-03:00"}
{"id":"mutranscriber-ajh","title":"Investigate transcription accuracy issues","description":"The transcription pipeline produces coherent output but with word accuracy issues.\n\n**Expected:** 'please visit LibriVox.org. Recording by Moira Fogarty. The Art of War by Sun Tzu. Translated by Lionel'\n\n**Actual:** 'visit leopoldoggs.org. Recording by Mario Fogarty. The Art of Borg by Sonzo. Translated by Lionel.'\n\n**Issues:**\n- Missing 'please' at start\n- 'LibriVox' → 'leopoldoggs'\n- 'Moira' → 'Mario'\n- 'The Art of War' → 'The Art of Borg'\n- 'Sun Tzu' → 'Sonzo'\n\n**Potential causes to investigate:**\n1. Prompt format - may not match what model expects\n2. Audio encoding quality - mel spectrogram or encoder issues\n3. Model configuration - check against reference implementation\n4. Temperature/sampling - currently using greedy, may need different params\n\n**Note:** BPE decoding, KV cache, and logits handling verified correct.","notes":"## Investigation Summary\n\n### Verified Correct:\n- Mel spectrogram parameters (n_fft=400, hop_length=160, n_mels=128, sample_rate=16000) match WhisperFeatureExtractor config\n- Whisper-style log-mel normalization (clamp to max-8.0, then (x+4.0)/4.0)\n- Audio encoder architecture (Conv2D 8x downsampling + Transformer + output projection)\n- KV cache and position tracking\n- Logits extraction and token generation\n- BPE decoding with ByteLevel decoder\n\n### Current Output:\n- Coherent sentences with correct structure\n- Word-level accuracy issues (LibriVox→leapfrogdocs, Moira→Moore, War→Frog)\n- No truncation or repetition problems\n\n### Potential Remaining Causes:\n1. Model inherent limitations (0.6B is smaller model)\n2. Prompt format - model may expect specific task tokens\n3. Subtle audio preprocessing differences vs reference implementation\n4. Missing language specification in prompt\n\n### Next Steps to Try:\n1. Test with 1.7B model to see if accuracy improves\n2. Compare audio encoder output statistics with reference implementation\n3. Try explicit language hint in prompt\n4. Investigate if model needs specific task tokens like Whisper uses","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:14:48.642431319-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:24:28.63139334-03:00","closed_at":"2026-02-02T14:24:28.63139334-03:00","close_reason":"Broken down into specific sub-tasks: mutranscriber-d36, mutranscriber-2lu, mutranscriber-sp4, mutranscriber-5dp"}
{"id":"mutranscriber-b84","title":"Fix logits shape handling in text generation","description":"Logits tensor dimension handling is fragile and may cause incorrect token selection.\n\n**Symptom:** Nonsense output, early stopping, wrong tokens selected\n\n**Root cause:** model.rs:354-360\n- forward_embeds() returns (batch=1, seq=1, vocab_size) - only last token logits\n- Subsequent forward() calls may have different shapes\n- No validation that tensor is 3D (batch, seq, vocab)\n- Indexing with logits.dim(1)? - 1 assumes specific shape\n\n**Fix needed:**\n- Validate logits tensor shape after each forward call\n- Ensure consistent handling between forward_embeds and forward\n- Add assertions for expected dimensions\n\n**Location:** src/model.rs lines 354-360, 391","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:18.426884629-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:14:40.078802398-03:00","closed_at":"2026-02-02T14:14:40.078802398-03:00","close_reason":"Verified implementation is correct - issues are transcription accuracy, not generation bugs"}
{"id":"mutranscriber-bjg","title":"Analyze current generation loop allocation patterns","description":"Profile and document all allocations in the generation loop:\n1. Map each Tensor::new, unsqueeze, clone call\n2. Identify which allocations are unavoidable vs. optimizable\n3. Measure allocation frequency and sizes\n4. Document the data flow between CPU and GPU\n\n**Files:** src/model.rs (generate function ~500-650)\n\n**Output:** Detailed analysis of allocation hotspots with line numbers\n\n**Parent:** mutranscriber-6ts","notes":"## Analysis Complete\n\n### Generation Loop Allocations (per token, lines 574-623)\n\n| Line | Operation | Allocations | Optimizable? |\n|------|-----------|-------------|--------------|\n| 576 | `logits.i(...)` | 1 tensor | Hard - needed for indexing |\n| 609/619 | `Tensor::new` + `unsqueeze` | 2 tensors | **Yes** - could reuse buffer |\n| 611/621 | `decoder.forward` | Internal | Handled by KV cache |\n\n### sample_token Allocations (per call, lines 669-746)\n\n**Greedy path (most common):**\n| Line | Operation | Allocations | Optimizable? |\n|------|-----------|-------------|--------------|\n| 692/695 | `logits.clone()` | 1 tensor | **Fixed** - fast path added |\n| 701 | `argmax` + `to_vec1` | 1 tensor + 1 vec | Hard - need result |\n\n**Sampling path (temperature \u003e 0):**\n- Additional softmax, potential top-k/top-p filtering tensors\n\n### Optimization Applied\nAdded fast path for greedy decoding without repetition penalty that:\n- Skips logits.clone() entirely\n- Calls argmax directly on borrowed logits\n- Saves ~100 tensor clones for typical 100-token generation\n\n### Remaining Bottlenecks (for future tasks)\n1. `Tensor::new` + `unsqueeze` per token - needs Candle research\n2. `logits.i(...)` indexing - may be unavoidable\n3. `argmax().to_vec1()` - extracting result to CPU","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T00:38:24.208478529-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T00:41:54.862785596-03:00","closed_at":"2026-02-03T00:41:54.862785596-03:00","close_reason":"Analysis complete with optimization: added fast path for greedy decoding that skips unnecessary logits.clone()"}
{"id":"mutranscriber-cmu","title":"Replace candle_nn::layer_norm with CUDA-compatible implementation","description":"Candle's built-in `candle_nn::layer_norm` doesn't have a CUDA kernel implementation, causing 'no cuda implementation for layer-norm' error when running on GPU.\n\n**Problem:**\n- audio_encoder.rs uses `candle_nn::layer_norm` (lines 11, 16, 150-174, 201-209)\n- This operation has no CUDA kernel in Candle 0.9\n- GPU inference fails with this error\n\n**Current usage in audio_encoder.rs:**\n- self_attn_layer_norm (per transformer layer)\n- final_layer_norm (per transformer layer)\n- ln_post (final output normalization)\n\n**Solution:**\nReplace with custom LayerNorm using basic ops that have CUDA support, similar to the RmsNorm implementation in qwen3_decoder.rs:\n\n```rust\nstruct LayerNormCustom {\n    weight: Tensor,\n    bias: Tensor,\n    eps: f64,\n}\n\nimpl LayerNormCustom {\n    fn forward(\u0026self, x: \u0026Tensor) -\u003e Result\u003cTensor\u003e {\n        let dtype = x.dtype();\n        let x = x.to_dtype(DType::F32)?;\n        let mean = x.mean_keepdim(D::Minus1)?;\n        let x_centered = x.broadcast_sub(\u0026mean)?;\n        let variance = x_centered.sqr()?.mean_keepdim(D::Minus1)?;\n        let x_norm = x_centered.broadcast_div(\u0026(variance + self.eps)?.sqrt()?)?;\n        x_norm.to_dtype(dtype)?.broadcast_mul(\u0026self.weight)?.broadcast_add(\u0026self.bias)\n    }\n}\n```\n\n**Alternative:**\nCheck if candle 0.10+ adds CUDA layer_norm support.\n\n**Location:** src/audio_encoder.rs","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T15:22:12.051101361-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T16:55:59.402721625-03:00","closed_at":"2026-02-02T16:55:59.402721625-03:00","close_reason":"Already implemented - custom LayerNorm was added in audio_encoder.rs as part of CUDA compatibility work"}
{"id":"mutranscriber-d36","title":"Test transcription accuracy with 1.7B model","description":"Test if the larger Qwen3-ASR-1.7B model produces better transcription accuracy than the 0.6B model.\n\n**Current 0.6B output:**\n'visit leapfrogdocs.org. Recording by Moore Froggy. The art of Frog by Sonzo. Translated by Lionel.'\n\n**Expected:**\n'please visit LibriVox.org. Recording by Moira Fogarty. The Art of War by Sun Tzu. Translated by Lionel'\n\n**Test approach:**\n1. Run integration test with --features large or model selection\n2. Compare word error rate between models\n3. Document if accuracy issues are model-size related or implementation-related\n\n**Location:** tests/integration_test.rs, src/model.rs","notes":"## Investigation Results\n\n**Config fixes applied:**\n- rope_theta: 10000 → 1000000 (correct for Qwen3)\n- max_position_embeddings: 4096 → 65536 (correct)\n- Added system message to prompt template\n\n**Finding:** Both 0.6B and 1.7B models produce subpar accuracy. 1.7B is actually worse, indicating implementation issue not model quality.\n\n**Current 0.6B output:** 'visit leapfrog.org. Recording by Morro Froggy, leapfrog.org by Sun Zhu, translated by Lionel.'\n**Expected:** 'please visit LibriVox.org. Recording by Moira Fogarty. The Art of War by Sun Tzu. Translated by Lionel'\n\n**Positives:**\n- 'Sun Zhu' is close to 'Sun Tzu'\n- 'translated by Lionel' is correct\n- Coherent sentence structure\n\n**Remaining issues likely in:**\n- Mel spectrogram extraction (subtle normalization differences)\n- Audio feature alignment with text embeddings\n- Need to compare with reference Python implementation","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.164466936-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T15:12:45.671019925-03:00","closed_at":"2026-02-02T15:12:45.671019925-03:00","close_reason":"Tested both models, found and fixed config issues (rope_theta, max_position_embeddings, system message). Accuracy still subpar - needs deeper investigation with reference impl."}
{"id":"mutranscriber-dku","title":"Improve special token handling logic","description":"The special token threshold logic is arbitrary and undocumented.\n\n**Current behavior:** model.rs:378-385\n- Tokens \u003e= 151643 are skipped but still fed to decoder\n- No documentation why 151643 is the threshold\n- May incorrectly skip needed tokens or preserve unwanted ones\n\n**Fix needed:**\n- Document the special token range (151643-151935 based on vocab_size 151936)\n- Use tokenizer's special token detection instead of hardcoded threshold\n- Consider if skipped tokens should be fed to decoder at all\n- Add tests for special token handling\n\n**Location:** src/model.rs lines 377-385","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:32.090953378-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T22:57:31.04695179-03:00","closed_at":"2026-02-02T22:57:31.04695179-03:00","close_reason":"Completed - added documented is_special_token method"}
{"id":"mutranscriber-f48","title":"Benchmark and enable CUDA GPU acceleration","description":"**Potential 5-10x speedup**\n\nCUDA/Metal features exist in Cargo.toml but GPU isn't being used by default.\n\n**Tasks:**\n1. Build with `cargo build --release --features cuda`\n2. Benchmark GPU vs CPU on test audio\n3. Compare: mel extraction, audio encoding, text generation\n4. Document GPU requirements and setup\n\n**Current CPU performance:**\n- Audio encoding: 1.5s\n- Text generation: 3.5s (14.9 tok/s)\n\n**Expected GPU performance:**\n- Text generation: potentially 100+ tok/s with GPU attention kernels\n\n**Location:** Cargo.toml, src/model.rs device selection","status":"in_progress","priority":0,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T00:49:41.20666729-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T00:57:01.84958139-03:00"}
{"id":"mutranscriber-gzb","title":"Fix CUDA layer-norm implementation missing","description":"When using the cuda feature, layer_norm from candle_nn fails with 'no cuda implementation for layer-norm'. The audio_encoder.rs uses candle_nn::layer_norm which lacks CUDA support, while qwen3_decoder.rs uses a custom RmsNorm with basic tensor ops that work fine on CUDA. Solution: implement a custom LayerNorm using basic tensor operations that have CUDA support.","status":"closed","priority":1,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T15:24:55.707727082-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T15:33:46.148222108-03:00","closed_at":"2026-02-02T15:33:46.148222108-03:00","close_reason":"Implemented custom LayerNorm and RoPE using basic tensor ops that have CUDA support"}
{"id":"mutranscriber-h5l","title":"Optimize KV cache to avoid O(n²) concatenation","description":"**Critical bottleneck in text generation (69% of total time)**\n\nThe KV cache in qwen3_decoder.rs:222-254 concatenates the entire sequence on every token:\n```rust\nlet k = Tensor::cat(\u0026[prev_k, \u0026k], 2)?;  // Copies all previous tokens\nlet v = Tensor::cat(\u0026[prev_v, \u0026v], 2)?;\n```\n\nFor 256 tokens: 1+2+...+256 = 32,896 token operations (O(n²))\n\n**Solutions (pick one):**\n1. Sliding window KV cache - only keep last N tokens\n2. Pre-allocate max-size KV buffer, use index tracking\n3. Ring buffer with modulo indexing\n\n**Estimated gain:** 30-50% of generation time\n\n**Location:** src/qwen3_decoder.rs lines 222-254","notes":"## Analysis\n\n### Problem\nTensor::cat copies all previous data + new data on each token, resulting in O(n²) total copies for n tokens.\n\n### Candle Constraint\nCandle tensors are **immutable** - no in-place updates possible. This means:\n- Cannot pre-allocate and fill a buffer\n- Cannot use slice assignment\n- Must create new tensors for any modification\n\n### Options Considered\n1. **Vec\u003cTensor\u003e approach**: Store each token separately, cat all at once\n   - Still O(n²) total copies (cat needed for every forward pass for attention)\n   - More memory fragmentation from many small tensors\n   - **Not better**\n\n2. **Pre-allocated buffer**: Allocate max size, fill incrementally\n   - **Not possible** with immutable tensors\n\n3. **Sliding window**: Keep only last N tokens\n   - Changes model behavior, may affect accuracy\n   - Would need to verify model supports this\n\n4. **Accept limitation**: For typical ASR (20-50 tokens), overhead is manageable\n   - 50 tokens: ~1275 element copies per layer\n   - With GPU, memory bandwidth makes this fast\n\n### Conclusion\nThe O(n²) KV cache complexity is inherent to Candle's design. Better ROI is:\n1. Enable GPU (much faster memory bandwidth)\n2. Keep outputs short (ASR typically is)\n\nAdded code comment documenting the limitation.","status":"closed","priority":0,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T00:49:33.358871185-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T00:55:14.504415645-03:00","closed_at":"2026-02-03T00:55:14.504415645-03:00","close_reason":"Not actionable: Candle tensors are immutable, cannot pre-allocate or update in-place. O(n²) complexity is inherent. Better ROI from GPU acceleration. Added documenting comment."}
{"id":"mutranscriber-jb9","title":"Add configurable generation parameters","description":"Currently generation parameters are hardcoded. Add a GenerationConfig struct for better control and debugging.\n\n**Current hardcoded values:**\n- max_new_tokens = 256 (model.rs:337)\n- Greedy decoding only (no temperature/top-k/top-p)\n- EOS token with fallback to hardcoded 151643\n\n**Proposed GenerationConfig:**\n- max_new_tokens: usize\n- temperature: Option\u003cf32\u003e\n- top_k: Option\u003cusize\u003e\n- top_p: Option\u003cf32\u003e\n- eos_token_id: u32\n- repetition_penalty: Option\u003cf32\u003e\n\n**Benefits:**\n- Runtime control without recompilation\n- Better debugging capability\n- Support for different generation strategies\n\n**Location:** src/model.rs, src/config.rs","status":"closed","priority":2,"issue_type":"feature","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:31.883644731-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T22:55:02.789013458-03:00","closed_at":"2026-02-02T22:55:02.789013458-03:00","close_reason":"Added GenerationConfig with temperature, top-k, top-p, repetition penalty support"}
{"id":"mutranscriber-k47","title":"Implement reusable token input buffer","description":"Create a pre-allocated tensor buffer for the next token input instead of creating new tensors each iteration.\n\n**Approach:**\n- Pre-allocate a (1, 1) tensor on the device\n- Use in-place copy or index assignment to update the token value\n- Verify Candle supports in-place tensor mutation\n\n**Challenge:** Candle tensors may be immutable. May need to investigate Tensor::copy_ or similar APIs.\n\n**Files:** src/model.rs, potentially src/qwen3_decoder.rs\n\n**Depends on:** mutranscriber-bjg, mutranscriber-nwb\n**Parent:** mutranscriber-6ts","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T00:38:34.912885984-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T00:44:21.637098978-03:00","closed_at":"2026-02-03T00:44:21.637098978-03:00","close_reason":"Not feasible: Candle tensors are immutable by design. No in-place operations available. Research (mutranscriber-nwb) confirmed our implementation already follows the idiomatic Candle pattern."}
{"id":"mutranscriber-k8l","title":"Remove debug sync points in transcribe()","description":"**Unnecessary GPU-CPU synchronization**\n\nAfter audio encoding, we compute stats that force GPU sync:\n```rust\nlet mean = features_flat.mean(0)?.to_scalar::\u003cf32\u003e()?;\nlet min = features_flat.min(0)?.to_scalar::\u003cf32\u003e()?;\nlet max = features_flat.max(0)?.to_scalar::\u003cf32\u003e()?;\n```\n\nThese are only used for debug logging but force 3 sync points.\n\n**Fix:** Move behind a debug flag or remove entirely. Only compute when RUST_LOG=debug.\n\n**Location:** src/model.rs:429-438","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T01:17:55.704725221-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T01:20:32.436755113-03:00","closed_at":"2026-02-03T01:20:32.436755113-03:00","close_reason":"Removed 3 to_scalar() calls that forced GPU-CPU sync after audio encoding"}
{"id":"mutranscriber-mmw","title":"Fix: ASR truncation - remove im_end from stop tokens","notes":"The model stops after one sentence because \u003c|im_end|\u003e (151645) is in stop_token_ids. For ASR, we should only stop on \u003c|endoftext|\u003e (151643), not on \u003c|im_end|\u003e which the model generates after each ChatML 'response' segment.","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-03T01:09:19.555340848-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T01:09:48.809191266-03:00","closed_at":"2026-02-03T01:09:48.809191266-03:00","close_reason":"Fixed: removed \u003c|im_end|\u003e from stop_token_ids so ASR continues past sentence boundaries"}
{"id":"mutranscriber-n7f","title":"GPU bottleneck: token sampling forces GPU-CPU sync every token","description":"**Root cause of 1% GPU utilization**\n\nThe generate() loop in model.rs calls `to_vec1()` after `argmax()` for EVERY token:\n- Line 714: `logits.argmax(...).to_vec1::\u003cu32\u003e()?[0]` (greedy path)\n- Line 743: Same for repetition penalty path\n- Lines 771, 787, 792, 821, 822: More `to_vec1()` in sampling\n\nEach `to_vec1()` call:\n1. Blocks until GPU finishes all pending work\n2. Transfers tensor data from GPU to CPU\n3. GPU sits idle while CPU processes token\n\n**This is a Candle limitation** - even their official LogitsProcessor does the same.\n\n**Possible mitigations:**\n1. Keep token tensor on GPU, use for embedding lookup directly\n2. Batch EOS checking (check every N tokens instead of every token)\n3. Speculative decoding - generate K tokens, then verify\n4. GPU-side sampling kernel (requires custom CUDA code)\n\n**Location:** src/model.rs:700-860 (sample_token and generate functions)","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-03T01:17:44.675528794-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T02:04:47.758373724-03:00","closed_at":"2026-02-03T02:04:47.758373724-03:00","close_reason":"Investigated: the bottleneck is CUDA kernel launch overhead (~420 launches per token), not sync overhead. See mutranscriber-wro notes for details."}
{"id":"mutranscriber-nwb","title":"Investigate Candle tensor reuse patterns","description":"Research how to efficiently reuse tensors in Candle:\n1. Check if Candle supports in-place operations (copy_, fill_, etc.)\n2. Look at how other Candle-based projects handle generation loops\n3. Review candle-transformers examples for generation patterns\n4. Determine if we need custom tensor management\n\n**References:**\n- candle-transformers text generation examples\n- Candle GitHub issues/discussions on tensor reuse\n\n**Parent:** mutranscriber-6ts","notes":"## Research Findings\n\n### Candle Tensor Immutability\n- **Tensors are immutable** - no PyTorch-style `copy_`, `fill_` in-place operations\n- Tensors use `Arc` internally, so cloning is cheap (reference count bump, not data copy)\n- Storage is refcounted independently for shape/stride-only modifications\n\n### Var Type for Mutability\n- `Var` wrapper exists with `set()` method for updating contents\n- Uses interior mutability pattern (no `\u0026mut self` needed)\n- **BUT**: Still requires creating a new tensor to pass to `set()`\n- Primarily designed for training/gradients, not inference optimization\n\n### Official Candle Examples Pattern\nThe llama example uses the same pattern we use:\n```rust\nlet ctxt = \u0026tokens[tokens.len().saturating_sub(context_size)..];\nlet input = Tensor::new(ctxt, \u0026device)?.unsqueeze(0)?;\n```\nThis creates new tensors each iteration - it's the idiomatic Candle approach.\n\n### Conclusion\n**Tensor reuse is not feasible with Candle's current API.** The immutable tensor design is intentional for safety and simplicity. Our current implementation follows the same pattern as official examples.\n\n### Alternative Approaches (for future consideration)\n1. Custom CUDA kernel that updates tensor data in-place\n2. Batch multiple tokens together (speculative decoding)\n3. Accept this as inherent Candle overhead - focus on other optimizations\n4. Consider switching to `tch` (Rust PyTorch bindings) if in-place ops are critical\n\n### References\n- https://docs.rs/candle-core/latest/candle_core/struct.Tensor.html\n- https://docs.rs/candle-core/latest/candle_core/struct.Var.html\n- https://github.com/huggingface/candle/blob/main/candle-examples/examples/llama/main.rs","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T00:38:28.122026336-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T00:44:17.071814054-03:00","closed_at":"2026-02-03T00:44:17.071814054-03:00","close_reason":"Research complete: Candle tensors are immutable by design. Tensor reuse not feasible with current API. Our implementation follows the same pattern as official candle examples."}
{"id":"mutranscriber-p20","title":"Optimize text generation loop to reduce per-token allocations","status":"closed","priority":1,"issue_type":"epic","owner":"me@cfcosta.com","created_at":"2026-02-03T00:38:14.71884203-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T00:38:54.872941785-03:00","closed_at":"2026-02-03T00:38:54.872941785-03:00","close_reason":"Duplicate - created without description"}
{"id":"mutranscriber-shi","title":"Benchmark generation loop before/after optimizations","description":"Create reproducible benchmarks to measure optimization impact:\n1. Time the generation phase separately from audio encoding\n2. Measure tokens/second throughput\n3. Compare CPU vs GPU performance\n4. Test with different sequence lengths (10, 50, 100, 200 tokens)\n\n**Metrics to capture:**\n- Total generation time\n- Time per token\n- Memory allocation count (if measurable)\n- Peak memory usage\n\n**Files:** Create benchmarks in tests/ or benches/\n\n**Depends on:** mutranscriber-k47\n**Parent:** mutranscriber-6ts","notes":"## Benchmark Results\n\n### Test File: tests/fixtures/test_audio.wav (~10 seconds audio)\n\n| Phase | Time | Percentage |\n|-------|------|------------|\n| Mel spectrogram | 57ms | 1% |\n| Audio encoding | 1.51s | 30% |\n| Text generation | 3.54s | 69% |\n| **Total** | **5.1s** | 100% |\n\n### Token Generation Metrics\n- Generated: 20 tokens\n- Time: 1.34s (generation loop only)\n- Rate: **14.9 tokens/sec**\n\n### Key Findings\n1. Text generation is the dominant bottleneck (70% of time)\n2. Audio encoding is significant but optimizable via GPU\n3. Mel spectrogram extraction is negligible (1%)\n4. Token generation rate of ~15 tok/s is typical for CPU inference\n\n### Timing Instrumentation Added\n- `src/model.rs:transcribe()` - logs mel/encode/generate breakdown\n- `src/model.rs:generate()` - logs tokens/sec metrics\n\nThese metrics help identify optimization targets and measure improvement impact.","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T00:38:41.026092782-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T00:46:27.540165335-03:00","closed_at":"2026-02-03T00:46:27.540165335-03:00","close_reason":"Timing instrumentation added. Benchmarks show: mel=1%, encode=30%, generate=69%. Token rate: 14.9 tok/s."}
{"id":"mutranscriber-sox","title":"Fix BPE subword token decoding - words split incorrectly","description":"The tokenizer decode process doesn't properly handle GPT-2 BPE subword joining. Tokens without Ġ prefix are subword continuations and should be joined without spaces, but currently each token becomes a separate word.\n\n**Symptom:** 'Foggy' outputs as 'F og gy', 'LibriVox' as 'Libre F og gy'\n\n**Root cause:** model.rs:401-423 only replaces Ġ with space but doesn't concatenate subword tokens.\n\n**Fix needed:**\n- Identify word-starting tokens (with Ġ prefix)\n- Join consecutive subword tokens (no prefix) without spaces\n- Handle byte-level fallback tokens\n- Normalize unusual Unicode characters\n\n**Location:** src/model.rs lines 401-423","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:17.915008435-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T13:47:54.433455206-03:00","closed_at":"2026-02-02T13:47:54.433455206-03:00","close_reason":"Fixed by adding ByteLevel decoder to tokenizer"}
{"id":"mutranscriber-sp4","title":"Try explicit language hint in transcription prompt","description":"The model may perform better with explicit language specification in the prompt.\n\n**Current prompt format:**\n`\u003c|im_start|\u003euser\\n\u003c|audio_start|\u003e[audio]\u003c|audio_end|\u003e\u003c|im_end|\u003e\\n\u003c|im_start|\u003eassistant\\n`\n\n**Try these variations:**\n1. Add 'Transcribe in English:' instruction\n2. Add system message with language context\n3. Try 'language English' prefix in user message\n4. Check if model has language-specific tokens (like Whisper's \u003c|en|\u003e)\n\n**Expected output format (from paper):**\n`\u003c|im_start|\u003eassistant language English\u003c|asr_text|\u003etranscription\u003c|im_end|\u003e`\n\nThe model outputs 'language X' prefix but we're not seeing it - may indicate prompt format issue.\n\n**Location:** src/model.rs generate() function","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.403584912-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T16:13:47.826850762-03:00","closed_at":"2026-02-02T16:13:47.826850762-03:00","close_reason":"Added explicit language hint (language English) and \u003casr_text\u003e token (151704) to the prompt. This improves output quality with proper capitalization (Please visit...) and better punctuation. The remaining word accuracy issues are related to audio encoding and should be addressed in issue 2lu."}
{"id":"mutranscriber-ttz","title":"Verify audio padding matches reference implementation","description":"The preprocessor_config.json shows:\n- chunk_length: 30 (seconds)\n- n_samples: 480000 (30 seconds at 16kHz)\n- padding_side: \"right\"\n- padding_value: 0.0\n\nOur implementation doesn't pad audio to a fixed length before processing. The official implementation may pad audio to 30 seconds before mel spectrogram extraction.\n\n**Impact:**\n- Different padding could affect normalization (the max value used for clamping)\n- Attention patterns might be trained with specific sequence lengths\n\n**To verify:**\n- Check if WhisperFeatureExtractor pads to n_samples by default\n- Compare mel spectrogram dimensions with and without padding\n- Test with padded vs unpadded audio\n\n**Location:** src/mel.rs, src/model.rs","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T16:59:02.952253173-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T19:22:00.855686542-03:00","closed_at":"2026-02-02T19:22:00.855686542-03:00","close_reason":"Implemented 30-second audio padding matching WhisperFeatureExtractor"}
{"id":"mutranscriber-ui2","title":"Fix: transcription still truncated on longer audio files","description":"Transcription only returns the first phrase for longer audio files (e.g. /tmp/example-big.webm). The earlier fix (mutranscriber-mmw) removed \u003c|im_end|\u003e from stop_token_ids, but the model is still stopping early. Need to investigate what token is causing early termination - could be \u003c|endoftext|\u003e (151643) being generated prematurely, or max_new_tokens (256) being too low for longer audio.","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-03T02:07:48.879534358-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T03:28:48.7501943-03:00","closed_at":"2026-02-03T03:28:48.7501943-03:00","close_reason":"Fixed: 1) Removed 30-second audio truncation in mel.rs (now only pads short audio). 2) Increased max_new_tokens from 256 to 4096. 3) Added automatic chunking in model.rs - audio longer than 30 seconds is split into chunks, each transcribed independently, and results concatenated. The audio encoder's max_source_positions=1500 limits each chunk to ~120s, so 30s chunks are safe and match Whisper's design."}
{"id":"mutranscriber-vr2","title":"Verify attention dropout is disabled during inference","description":"The audio encoder config shows dropout: 0.0, but we should verify that:\n\n1. Dropout is actually disabled during inference\n2. No accidental dropout is being applied\n\nThe config shows activation_dropout: 0 and attention_dropout: 0 (from HuggingFace config.json) but we should verify our implementation matches.\n\n**Location:** src/audio_encoder.rs, src/qwen3_decoder.rs","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T16:59:18.695259213-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T23:01:31.600150793-03:00","closed_at":"2026-02-02T23:01:31.600150793-03:00","close_reason":"Verified: No dropout implemented in audio_encoder.rs or qwen3_decoder.rs. This matches the HuggingFace config (attention_dropout=0, activation_dropout=0) and is correct for inference."}
{"id":"mutranscriber-w1h","title":"Fix Nix flake LD_LIBRARY_PATH for CUDA driver","description":"The Nix devshell sets CUDA_HOME and CUDA_PATH but doesn't include /run/opengl-driver/lib/ in LD_LIBRARY_PATH. This causes CUDA_ERROR_STUB_LIBRARY because libcuda.so from the NVIDIA driver isn't found.\n\n**Problem:**\n- CUDA toolkit libraries are available via the cuda symlink\n- libcuda.so (the driver) lives in /run/opengl-driver/lib/ on NixOS\n- LD_LIBRARY_PATH is empty in the devshell\n\n**Fix needed in flake.nix:**\nAdd to devShells.default:\n```nix\nLD_LIBRARY_PATH = lib.makeLibraryPath [\n  \"/run/opengl-driver\"\n];\n```\n\nOr use:\n```nix\nshellHook = ''\n  export LD_LIBRARY_PATH=/run/opengl-driver/lib:$LD_LIBRARY_PATH\n'';\n```\n\n**Verification:**\n```bash\nLD_LIBRARY_PATH=/run/opengl-driver/lib cargo run --features cuda ...\n# Should initialize CUDA successfully\n```\n\n**Location:** flake.nix lines 96-123","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T15:22:11.959916181-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T00:01:00.27331973-03:00","closed_at":"2026-02-03T00:01:00.27331973-03:00","close_reason":"Fixed in the CUDA package build - LD_LIBRARY_PATH now includes /run/opengl-driver/lib"}
{"id":"mutranscriber-wes","title":"Avoid redundant dtype conversions in LayerNorm","description":"**2-5% of audio encoder time**\n\naudio_encoder.rs:39-57 LayerNorm converts to F32 and back for every call:\n```rust\nfn forward(\\\u0026self, x: \\\u0026Tensor) -\u003e Result\u003cTensor\u003e {\n    let dtype = x.dtype();\n    let x = x.to_dtype(DType::F32)?;  // Convert to F32\n    // ... computation ...\n    x.to_dtype(dtype)?  // Convert back\n}\n```\n\nWith 18 encoder layers calling LayerNorm multiple times, this adds overhead.\n\n**Fix:** \n- Skip conversion if input is already F32\n- Consider keeping computation in input dtype when possible\n\n**Location:** src/audio_encoder.rs lines 39-57","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T00:49:55.356301466-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T02:10:27.789531839-03:00","closed_at":"2026-02-03T02:10:27.789531839-03:00","close_reason":"Skip dtype conversion when input is already F32. Applied to both LayerNorm (audio_encoder) and RmsNorm (qwen3_decoder)."}
{"id":"mutranscriber-wro","title":"Keep next token tensor on GPU for embedding lookup","description":"**Reduce GPU-CPU round trips during generation**\n\nCurrently in generate(), we:\n1. Compute argmax on GPU\n2. Transfer token ID to CPU with `to_vec1()`\n3. Create new tensor from token ID\n4. Transfer back to GPU for embedding\n\n**Optimization:** Keep the argmax result as a tensor on GPU and use it directly:\n```rust\n// Before (current):\nlet next_token = logits.argmax(D::Minus1)?.to_vec1::\u003cu32\u003e()?[0];\nlet next_input = Tensor::new(\u0026[next_token], \u0026device)?.unsqueeze(0)?;\nlogits = self.decoder.forward(\u0026next_input, position)?;\n\n// After (optimized):\nlet next_token_tensor = logits.argmax(D::Minus1)?; // stays on GPU\n// Use gather/index_select for embedding lookup instead of forward()\n```\n\n**Challenge:** Still need to check EOS, which requires reading the token value.\nCould batch EOS checks every N tokens.\n\n**Location:** src/model.rs:595-644 (generate loop)","notes":"## Investigation Results\n\nTested two approaches:\n1. **Batched greedy**: Generate 16 tokens on GPU before syncing → 9.5 tok/s (33% regression!)\n2. **GPU tensor reuse**: Keep argmax on GPU, reshape for next forward → 13.3 tok/s (no improvement)\n\n### Root Cause\nThe bottleneck is **CUDA kernel launch overhead**, not data transfer:\n- 28 decoder layers × ~15 kernels/layer = 420+ kernel launches per token\n- Each kernel launch costs ~10µs regardless of data size\n- Batch size 1 means each kernel operates on (1,1) tensors\n- GPU utilization is low because kernels finish faster than launch overhead\n\n### Solutions That Would Help (Not Available in Candle)\n- CUDA graph capture (run entire forward as single launch)\n- Operator fusion (merge small kernels)\n- Flash attention with fused kernels\n\n### Conclusion\nNot actionable within Candle's current architecture. Low GPU utilization for autoregressive generation with batch_size=1 is a known limitation.","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T01:18:46.280954761-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T02:04:40.237177837-03:00","closed_at":"2026-02-03T02:04:40.237177837-03:00","close_reason":"Not actionable: CUDA kernel launch overhead is the bottleneck, not data transfer. Tested GPU tensor reuse and batched generation - neither improves performance."}
{"id":"mutranscriber-xyg","title":"Fix KV cache position tracking in generation loop","description":"The autoregressive generation loop has broken position tracking that corrupts the KV cache, causing missing content and incomplete sentences.\n\n**Symptom:** Transcription cuts off early, missing words, incomplete sentences\n\n**Root cause:** model.rs:351-392\n- position starts at total_prompt_len but KV cache was built with offset=0\n- forward_embeds() processes full sequence, then forward() is called iteratively\n- Position offset for RoPE doesn't align with KV cache state\n- KV cache isn't properly extended for newly generated tokens\n\n**Fix needed:**\n- Clear and rebuild KV cache at generation start OR\n- Properly track sequence position for RoPE relative to KV cache\n- Ensure each new token is processed with correct context\n\n**Location:** src/model.rs lines 351-392, src/qwen3_decoder.rs","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:18.139069045-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:14:40.077226988-03:00","closed_at":"2026-02-02T14:14:40.077226988-03:00","close_reason":"Verified implementation is correct - issues are transcription accuracy, not generation bugs"}
{"id":"mutranscriber-yhz","title":"Remove unnecessary .contiguous() calls in audio encoder attention","description":"**5-15% of audio encoder time**\n\naudio_encoder.rs:197-223 has multiple unnecessary .contiguous() calls:\n```rust\nlet q = q.transpose(1, 2)?.contiguous()?;\nlet k = k.transpose(1, 2)?.contiguous()?;\nlet v = v.transpose(1, 2)?.contiguous()?;\nlet attn_weights = q.matmul(\u0026k.transpose(2, 3)?.contiguous()?)?;\nlet attn_output = attn_output.transpose(1, 2)?.contiguous()?;\n```\n\nEach .contiguous() forces a tensor copy. With 18 encoder layers, this adds up.\n\n**Fix:** Test if single .contiguous() after all reshapes is sufficient, or if matmul works with non-contiguous tensors.\n\n**Location:** src/audio_encoder.rs lines 197-223","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T00:49:48.969677853-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T02:09:05.728208521-03:00","closed_at":"2026-02-03T02:09:05.728208521-03:00","close_reason":"Removed 4 unnecessary .contiguous() calls per layer (72 total across 18 layers). CUBLAS handles transposed inputs natively. Kept the one before reshape which is required."}
{"id":"mutranscriber-z2v","title":"Evaluate speculative decoding for batch efficiency","description":"Investigate if speculative decoding could improve throughput:\n1. Generate multiple candidate tokens in parallel\n2. Verify candidates against the model\n3. Accept valid prefix, reject and regenerate from divergence point\n\n**Trade-offs:**\n- More computation per step but fewer steps overall\n- Requires careful implementation to maintain correctness\n- May not be worth it for small models (0.6B)\n\n**Priority:** Lower - only if simpler optimizations don't suffice\n\n**Parent:** mutranscriber-6ts","status":"closed","priority":3,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-03T00:38:46.989675682-03:00","created_by":"Cainã Costa","updated_at":"2026-02-03T00:46:33.16855542-03:00","closed_at":"2026-02-03T00:46:33.16855542-03:00","close_reason":"Lower priority - tensor reuse not feasible with Candle, so speculative decoding won't provide significant benefit. Current optimizations are sufficient."}
