{"id":"mutranscriber-0cr","title":"Investigate 1.7B model accuracy issues","description":"The 1.7B model produces significantly worse transcription than the 0.6B model despite using the same codebase and preprocessing.\n\n**Expected (ground truth):**\n\"Please visit LibriVox.org. Recording by Moira Fogarty. The Art of War by Sun Tzu, translated by Lionel Giles.\"\n\n**0.6B output (good):**\n\"Please visit LibriVox.org. Recording by Mario Vitti, The Art of War by Sun Tzu, translated by Lionel.\"\n\n**1.7B output (broken):**\n\"Please visit believerbox.com. Recorded by More Property. The Art of War by Sun Tzu. Translated by Lionel Giles.\"\n\n## Investigation Results\n\n### Config verification (PASSED)\nBoth models' configs match official HuggingFace config.json:\n- 1.7B audio: d_model=1024, encoder_layers=24, heads=16, ffn_dim=4096, output_dim=2048\n- 1.7B text: hidden_size=2048, num_layers=28, heads=16, kv_heads=8, intermediate=6144\n\n### Audio features comparison\n| Metric | 0.6B | 1.7B |\n|--------|------|------|\n| Shape | [1, 375, 1024] | [1, 375, 2048] |\n| Mean | 0.000536 | 0.000360 |\n| Std | 0.021898 | 0.019995 |\n| Min | -0.110851 | -0.165795 |\n| Max | 0.099055 | 0.164644 |\n\nThe 1.7B has ~50% larger range (min/max) but similar statistics overall.\n\n### Weight loading (PASSED)\n- Both shards load correctly (654 + 54 weights)\n- Weight names match expected paths\n\n## Remaining hypotheses\n1. **Numerical precision**: bfloat16 -\u003e f32 conversion may affect 1.7B more\n2. **Layer-by-layer divergence**: Need to compare intermediate activations with Python\n3. **Attention pattern differences**: Larger model may require different handling\n4. **Output projection scaling**: May need normalization between encoder and decoder\n\n## Next steps\nCompare layer-by-layer tensor outputs between Rust and Python using the reference_diagnostics.py framework.\n\n**Location:** src/config.rs, src/model.rs, src/audio_encoder.rs","status":"open","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T19:29:03.087573006-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T22:49:06.104005403-03:00"}
{"id":"mutranscriber-2lu","title":"Compare audio encoder output with reference implementation","description":"Compare the audio encoder output statistics (mean, std, min, max) with the official Qwen3-ASR Python implementation to identify any preprocessing or encoding differences.\n\n**Approach:**\n1. Set up reference implementation (Python with transformers/qwen_asr)\n2. Process same test audio file through both implementations\n3. Compare mel spectrogram values\n4. Compare audio encoder output tensor statistics\n5. Identify any normalization or scaling differences\n\n**Files to compare:**\n- Mel spectrogram output\n- Audio encoder output (before LLM embedding)\n- Token embeddings\n\n**Location:** src/mel.rs, src/audio_encoder.rs","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.291548413-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T16:35:36.687279221-03:00","closed_at":"2026-02-02T16:35:36.687279221-03:00","close_reason":"Added diagnostic tools for comparing Rust vs Python implementations. Created: diagnostics/reference_diagnostics.py (Python reference), diagnostics/compare_diagnostics.py (comparison tool), diagnostics/README.md (usage guide). Set MUTRANSCRIBER_DIAGNOSTICS env var to dump tensor stats from Rust. Current Rust values documented for test audio."}
{"id":"mutranscriber-4b0","title":"Add tensor shape validation in model pipeline","description":"Add assertions to validate tensor shapes throughout the pipeline to catch silent failures early.\n\n**Locations needing validation:**\n- Embedding concatenation (model.rs:328) - verify pre_embed, audio_features, post_embed have same hidden_size\n- Token input preparation (model.rs:390) - verify next_input shape\n- Logits extraction (model.rs:355) - verify 3D shape before indexing\n\n**Benefits:**\n- Catch shape mismatches before they cause cryptic errors\n- Easier debugging\n- Better error messages\n\n**Location:** src/model.rs","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:31.722775716-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T22:51:06.373916677-03:00","closed_at":"2026-02-02T22:51:06.373916677-03:00","close_reason":"Added shape validation for audio features, embeddings, and logits"}
{"id":"mutranscriber-5dp","title":"Investigate task-specific tokens for ASR","description":"Whisper uses task-specific tokens like \u003c|transcribe|\u003e, \u003c|translate|\u003e, \u003c|notimestamps|\u003e. Qwen3-ASR may have similar tokens that need to be included in the prompt.\n\n**Investigation needed:**\n1. Check tokenizer vocabulary for task tokens (transcribe, asr, speech, etc.)\n2. Look for \u003c|asr_text|\u003e token ID (mentioned in paper output format)\n3. Check if there are timestamp-related tokens\n4. Review Qwen3-ASR toolkit source for prompt construction\n\n**Potentially relevant token IDs:**\n- 151643: \u003c|endoftext|\u003e\n- 151644: \u003c|im_start|\u003e\n- 151645: \u003c|im_end|\u003e\n- 151669: \u003c|audio_start|\u003e\n- 151670: \u003c|audio_end|\u003e\n- 151676: audio placeholder\n- ???: \u003c|asr_text|\u003e (from paper, ID unknown)\n\n**Location:** src/model.rs, vocab.json from HuggingFace","status":"open","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.514037461-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:24:23.514037461-03:00"}
{"id":"mutranscriber-8nz","title":"Verify mel filterbank matches WhisperFeatureExtractor","description":"Our mel filterbank implementation may differ from WhisperFeatureExtractor.\n\n**Potential issues identified:**\n\n1. **Mel scale formula**: We use HTK formula (2595 * log10(1 + f/700)) but WhisperFeatureExtractor might use a different variant or breakpoint.\n\n2. **FFT bin calculation**: We use `((n_fft + 1) * f / sample_rate).floor()` but the standard formula is `round(f * n_fft / sample_rate)`. The +1 is suspicious.\n\n3. **Slaney normalization**: We apply area normalization but need to verify it matches exactly.\n\n**To verify:**\n- Compare our mel filterbank matrix (first few rows) against WhisperFeatureExtractor\n- Check if WhisperFeatureExtractor uses htk=True or False\n- Verify the frequency-to-bin mapping\n\n**Current behavior:**\nOutput sounds phonetically similar but wrong words (\"leapfrog\" vs \"LibriVox\")\n\n**Location:** src/mel.rs create_mel_filterbank()","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T16:58:55.079274889-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T18:18:07.993744894-03:00","closed_at":"2026-02-02T18:18:07.993744894-03:00","close_reason":"Fixed mel filterbank construction using floating-point interpolation"}
{"id":"mutranscriber-ajh","title":"Investigate transcription accuracy issues","description":"The transcription pipeline produces coherent output but with word accuracy issues.\n\n**Expected:** 'please visit LibriVox.org. Recording by Moira Fogarty. The Art of War by Sun Tzu. Translated by Lionel'\n\n**Actual:** 'visit leopoldoggs.org. Recording by Mario Fogarty. The Art of Borg by Sonzo. Translated by Lionel.'\n\n**Issues:**\n- Missing 'please' at start\n- 'LibriVox' → 'leopoldoggs'\n- 'Moira' → 'Mario'\n- 'The Art of War' → 'The Art of Borg'\n- 'Sun Tzu' → 'Sonzo'\n\n**Potential causes to investigate:**\n1. Prompt format - may not match what model expects\n2. Audio encoding quality - mel spectrogram or encoder issues\n3. Model configuration - check against reference implementation\n4. Temperature/sampling - currently using greedy, may need different params\n\n**Note:** BPE decoding, KV cache, and logits handling verified correct.","notes":"## Investigation Summary\n\n### Verified Correct:\n- Mel spectrogram parameters (n_fft=400, hop_length=160, n_mels=128, sample_rate=16000) match WhisperFeatureExtractor config\n- Whisper-style log-mel normalization (clamp to max-8.0, then (x+4.0)/4.0)\n- Audio encoder architecture (Conv2D 8x downsampling + Transformer + output projection)\n- KV cache and position tracking\n- Logits extraction and token generation\n- BPE decoding with ByteLevel decoder\n\n### Current Output:\n- Coherent sentences with correct structure\n- Word-level accuracy issues (LibriVox→leapfrogdocs, Moira→Moore, War→Frog)\n- No truncation or repetition problems\n\n### Potential Remaining Causes:\n1. Model inherent limitations (0.6B is smaller model)\n2. Prompt format - model may expect specific task tokens\n3. Subtle audio preprocessing differences vs reference implementation\n4. Missing language specification in prompt\n\n### Next Steps to Try:\n1. Test with 1.7B model to see if accuracy improves\n2. Compare audio encoder output statistics with reference implementation\n3. Try explicit language hint in prompt\n4. Investigate if model needs specific task tokens like Whisper uses","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:14:48.642431319-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:24:28.63139334-03:00","closed_at":"2026-02-02T14:24:28.63139334-03:00","close_reason":"Broken down into specific sub-tasks: mutranscriber-d36, mutranscriber-2lu, mutranscriber-sp4, mutranscriber-5dp"}
{"id":"mutranscriber-b84","title":"Fix logits shape handling in text generation","description":"Logits tensor dimension handling is fragile and may cause incorrect token selection.\n\n**Symptom:** Nonsense output, early stopping, wrong tokens selected\n\n**Root cause:** model.rs:354-360\n- forward_embeds() returns (batch=1, seq=1, vocab_size) - only last token logits\n- Subsequent forward() calls may have different shapes\n- No validation that tensor is 3D (batch, seq, vocab)\n- Indexing with logits.dim(1)? - 1 assumes specific shape\n\n**Fix needed:**\n- Validate logits tensor shape after each forward call\n- Ensure consistent handling between forward_embeds and forward\n- Add assertions for expected dimensions\n\n**Location:** src/model.rs lines 354-360, 391","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:18.426884629-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:14:40.078802398-03:00","closed_at":"2026-02-02T14:14:40.078802398-03:00","close_reason":"Verified implementation is correct - issues are transcription accuracy, not generation bugs"}
{"id":"mutranscriber-cmu","title":"Replace candle_nn::layer_norm with CUDA-compatible implementation","description":"Candle's built-in `candle_nn::layer_norm` doesn't have a CUDA kernel implementation, causing 'no cuda implementation for layer-norm' error when running on GPU.\n\n**Problem:**\n- audio_encoder.rs uses `candle_nn::layer_norm` (lines 11, 16, 150-174, 201-209)\n- This operation has no CUDA kernel in Candle 0.9\n- GPU inference fails with this error\n\n**Current usage in audio_encoder.rs:**\n- self_attn_layer_norm (per transformer layer)\n- final_layer_norm (per transformer layer)\n- ln_post (final output normalization)\n\n**Solution:**\nReplace with custom LayerNorm using basic ops that have CUDA support, similar to the RmsNorm implementation in qwen3_decoder.rs:\n\n```rust\nstruct LayerNormCustom {\n    weight: Tensor,\n    bias: Tensor,\n    eps: f64,\n}\n\nimpl LayerNormCustom {\n    fn forward(\u0026self, x: \u0026Tensor) -\u003e Result\u003cTensor\u003e {\n        let dtype = x.dtype();\n        let x = x.to_dtype(DType::F32)?;\n        let mean = x.mean_keepdim(D::Minus1)?;\n        let x_centered = x.broadcast_sub(\u0026mean)?;\n        let variance = x_centered.sqr()?.mean_keepdim(D::Minus1)?;\n        let x_norm = x_centered.broadcast_div(\u0026(variance + self.eps)?.sqrt()?)?;\n        x_norm.to_dtype(dtype)?.broadcast_mul(\u0026self.weight)?.broadcast_add(\u0026self.bias)\n    }\n}\n```\n\n**Alternative:**\nCheck if candle 0.10+ adds CUDA layer_norm support.\n\n**Location:** src/audio_encoder.rs","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T15:22:12.051101361-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T16:55:59.402721625-03:00","closed_at":"2026-02-02T16:55:59.402721625-03:00","close_reason":"Already implemented - custom LayerNorm was added in audio_encoder.rs as part of CUDA compatibility work"}
{"id":"mutranscriber-d36","title":"Test transcription accuracy with 1.7B model","description":"Test if the larger Qwen3-ASR-1.7B model produces better transcription accuracy than the 0.6B model.\n\n**Current 0.6B output:**\n'visit leapfrogdocs.org. Recording by Moore Froggy. The art of Frog by Sonzo. Translated by Lionel.'\n\n**Expected:**\n'please visit LibriVox.org. Recording by Moira Fogarty. The Art of War by Sun Tzu. Translated by Lionel'\n\n**Test approach:**\n1. Run integration test with --features large or model selection\n2. Compare word error rate between models\n3. Document if accuracy issues are model-size related or implementation-related\n\n**Location:** tests/integration_test.rs, src/model.rs","notes":"## Investigation Results\n\n**Config fixes applied:**\n- rope_theta: 10000 → 1000000 (correct for Qwen3)\n- max_position_embeddings: 4096 → 65536 (correct)\n- Added system message to prompt template\n\n**Finding:** Both 0.6B and 1.7B models produce subpar accuracy. 1.7B is actually worse, indicating implementation issue not model quality.\n\n**Current 0.6B output:** 'visit leapfrog.org. Recording by Morro Froggy, leapfrog.org by Sun Zhu, translated by Lionel.'\n**Expected:** 'please visit LibriVox.org. Recording by Moira Fogarty. The Art of War by Sun Tzu. Translated by Lionel'\n\n**Positives:**\n- 'Sun Zhu' is close to 'Sun Tzu'\n- 'translated by Lionel' is correct\n- Coherent sentence structure\n\n**Remaining issues likely in:**\n- Mel spectrogram extraction (subtle normalization differences)\n- Audio feature alignment with text embeddings\n- Need to compare with reference Python implementation","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.164466936-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T15:12:45.671019925-03:00","closed_at":"2026-02-02T15:12:45.671019925-03:00","close_reason":"Tested both models, found and fixed config issues (rope_theta, max_position_embeddings, system message). Accuracy still subpar - needs deeper investigation with reference impl."}
{"id":"mutranscriber-dku","title":"Improve special token handling logic","description":"The special token threshold logic is arbitrary and undocumented.\n\n**Current behavior:** model.rs:378-385\n- Tokens \u003e= 151643 are skipped but still fed to decoder\n- No documentation why 151643 is the threshold\n- May incorrectly skip needed tokens or preserve unwanted ones\n\n**Fix needed:**\n- Document the special token range (151643-151935 based on vocab_size 151936)\n- Use tokenizer's special token detection instead of hardcoded threshold\n- Consider if skipped tokens should be fed to decoder at all\n- Add tests for special token handling\n\n**Location:** src/model.rs lines 377-385","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:32.090953378-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T22:57:31.04695179-03:00","closed_at":"2026-02-02T22:57:31.04695179-03:00","close_reason":"Completed - added documented is_special_token method"}
{"id":"mutranscriber-gzb","title":"Fix CUDA layer-norm implementation missing","description":"When using the cuda feature, layer_norm from candle_nn fails with 'no cuda implementation for layer-norm'. The audio_encoder.rs uses candle_nn::layer_norm which lacks CUDA support, while qwen3_decoder.rs uses a custom RmsNorm with basic tensor ops that work fine on CUDA. Solution: implement a custom LayerNorm using basic tensor operations that have CUDA support.","status":"closed","priority":1,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T15:24:55.707727082-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T15:33:46.148222108-03:00","closed_at":"2026-02-02T15:33:46.148222108-03:00","close_reason":"Implemented custom LayerNorm and RoPE using basic tensor ops that have CUDA support"}
{"id":"mutranscriber-jb9","title":"Add configurable generation parameters","description":"Currently generation parameters are hardcoded. Add a GenerationConfig struct for better control and debugging.\n\n**Current hardcoded values:**\n- max_new_tokens = 256 (model.rs:337)\n- Greedy decoding only (no temperature/top-k/top-p)\n- EOS token with fallback to hardcoded 151643\n\n**Proposed GenerationConfig:**\n- max_new_tokens: usize\n- temperature: Option\u003cf32\u003e\n- top_k: Option\u003cusize\u003e\n- top_p: Option\u003cf32\u003e\n- eos_token_id: u32\n- repetition_penalty: Option\u003cf32\u003e\n\n**Benefits:**\n- Runtime control without recompilation\n- Better debugging capability\n- Support for different generation strategies\n\n**Location:** src/model.rs, src/config.rs","status":"closed","priority":2,"issue_type":"feature","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:31.883644731-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T22:55:02.789013458-03:00","closed_at":"2026-02-02T22:55:02.789013458-03:00","close_reason":"Added GenerationConfig with temperature, top-k, top-p, repetition penalty support"}
{"id":"mutranscriber-sox","title":"Fix BPE subword token decoding - words split incorrectly","description":"The tokenizer decode process doesn't properly handle GPT-2 BPE subword joining. Tokens without Ġ prefix are subword continuations and should be joined without spaces, but currently each token becomes a separate word.\n\n**Symptom:** 'Foggy' outputs as 'F og gy', 'LibriVox' as 'Libre F og gy'\n\n**Root cause:** model.rs:401-423 only replaces Ġ with space but doesn't concatenate subword tokens.\n\n**Fix needed:**\n- Identify word-starting tokens (with Ġ prefix)\n- Join consecutive subword tokens (no prefix) without spaces\n- Handle byte-level fallback tokens\n- Normalize unusual Unicode characters\n\n**Location:** src/model.rs lines 401-423","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:17.915008435-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T13:47:54.433455206-03:00","closed_at":"2026-02-02T13:47:54.433455206-03:00","close_reason":"Fixed by adding ByteLevel decoder to tokenizer"}
{"id":"mutranscriber-sp4","title":"Try explicit language hint in transcription prompt","description":"The model may perform better with explicit language specification in the prompt.\n\n**Current prompt format:**\n`\u003c|im_start|\u003euser\\n\u003c|audio_start|\u003e[audio]\u003c|audio_end|\u003e\u003c|im_end|\u003e\\n\u003c|im_start|\u003eassistant\\n`\n\n**Try these variations:**\n1. Add 'Transcribe in English:' instruction\n2. Add system message with language context\n3. Try 'language English' prefix in user message\n4. Check if model has language-specific tokens (like Whisper's \u003c|en|\u003e)\n\n**Expected output format (from paper):**\n`\u003c|im_start|\u003eassistant language English\u003c|asr_text|\u003etranscription\u003c|im_end|\u003e`\n\nThe model outputs 'language X' prefix but we're not seeing it - may indicate prompt format issue.\n\n**Location:** src/model.rs generate() function","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.403584912-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T16:13:47.826850762-03:00","closed_at":"2026-02-02T16:13:47.826850762-03:00","close_reason":"Added explicit language hint (language English) and \u003casr_text\u003e token (151704) to the prompt. This improves output quality with proper capitalization (Please visit...) and better punctuation. The remaining word accuracy issues are related to audio encoding and should be addressed in issue 2lu."}
{"id":"mutranscriber-ttz","title":"Verify audio padding matches reference implementation","description":"The preprocessor_config.json shows:\n- chunk_length: 30 (seconds)\n- n_samples: 480000 (30 seconds at 16kHz)\n- padding_side: \"right\"\n- padding_value: 0.0\n\nOur implementation doesn't pad audio to a fixed length before processing. The official implementation may pad audio to 30 seconds before mel spectrogram extraction.\n\n**Impact:**\n- Different padding could affect normalization (the max value used for clamping)\n- Attention patterns might be trained with specific sequence lengths\n\n**To verify:**\n- Check if WhisperFeatureExtractor pads to n_samples by default\n- Compare mel spectrogram dimensions with and without padding\n- Test with padded vs unpadded audio\n\n**Location:** src/mel.rs, src/model.rs","status":"closed","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T16:59:02.952253173-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T19:22:00.855686542-03:00","closed_at":"2026-02-02T19:22:00.855686542-03:00","close_reason":"Implemented 30-second audio padding matching WhisperFeatureExtractor"}
{"id":"mutranscriber-vr2","title":"Verify attention dropout is disabled during inference","description":"The audio encoder config shows dropout: 0.0, but we should verify that:\n\n1. Dropout is actually disabled during inference\n2. No accidental dropout is being applied\n\nThe config shows activation_dropout: 0 and attention_dropout: 0 (from HuggingFace config.json) but we should verify our implementation matches.\n\n**Location:** src/audio_encoder.rs, src/qwen3_decoder.rs","status":"open","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T16:59:18.695259213-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T16:59:18.695259213-03:00"}
{"id":"mutranscriber-w1h","title":"Fix Nix flake LD_LIBRARY_PATH for CUDA driver","description":"The Nix devshell sets CUDA_HOME and CUDA_PATH but doesn't include /run/opengl-driver/lib/ in LD_LIBRARY_PATH. This causes CUDA_ERROR_STUB_LIBRARY because libcuda.so from the NVIDIA driver isn't found.\n\n**Problem:**\n- CUDA toolkit libraries are available via the cuda symlink\n- libcuda.so (the driver) lives in /run/opengl-driver/lib/ on NixOS\n- LD_LIBRARY_PATH is empty in the devshell\n\n**Fix needed in flake.nix:**\nAdd to devShells.default:\n```nix\nLD_LIBRARY_PATH = lib.makeLibraryPath [\n  \"/run/opengl-driver\"\n];\n```\n\nOr use:\n```nix\nshellHook = ''\n  export LD_LIBRARY_PATH=/run/opengl-driver/lib:$LD_LIBRARY_PATH\n'';\n```\n\n**Verification:**\n```bash\nLD_LIBRARY_PATH=/run/opengl-driver/lib cargo run --features cuda ...\n# Should initialize CUDA successfully\n```\n\n**Location:** flake.nix lines 96-123","status":"open","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T15:22:11.959916181-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T15:22:11.959916181-03:00"}
{"id":"mutranscriber-xyg","title":"Fix KV cache position tracking in generation loop","description":"The autoregressive generation loop has broken position tracking that corrupts the KV cache, causing missing content and incomplete sentences.\n\n**Symptom:** Transcription cuts off early, missing words, incomplete sentences\n\n**Root cause:** model.rs:351-392\n- position starts at total_prompt_len but KV cache was built with offset=0\n- forward_embeds() processes full sequence, then forward() is called iteratively\n- Position offset for RoPE doesn't align with KV cache state\n- KV cache isn't properly extended for newly generated tokens\n\n**Fix needed:**\n- Clear and rebuild KV cache at generation start OR\n- Properly track sequence position for RoPE relative to KV cache\n- Ensure each new token is processed with correct context\n\n**Location:** src/model.rs lines 351-392, src/qwen3_decoder.rs","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:18.139069045-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:14:40.077226988-03:00","closed_at":"2026-02-02T14:14:40.077226988-03:00","close_reason":"Verified implementation is correct - issues are transcription accuracy, not generation bugs"}
