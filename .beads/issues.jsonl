{"id":"mutranscriber-2lu","title":"Compare audio encoder output with reference implementation","description":"Compare the audio encoder output statistics (mean, std, min, max) with the official Qwen3-ASR Python implementation to identify any preprocessing or encoding differences.\n\n**Approach:**\n1. Set up reference implementation (Python with transformers/qwen_asr)\n2. Process same test audio file through both implementations\n3. Compare mel spectrogram values\n4. Compare audio encoder output tensor statistics\n5. Identify any normalization or scaling differences\n\n**Files to compare:**\n- Mel spectrogram output\n- Audio encoder output (before LLM embedding)\n- Token embeddings\n\n**Location:** src/mel.rs, src/audio_encoder.rs","status":"open","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.291548413-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:24:23.291548413-03:00"}
{"id":"mutranscriber-4b0","title":"Add tensor shape validation in model pipeline","description":"Add assertions to validate tensor shapes throughout the pipeline to catch silent failures early.\n\n**Locations needing validation:**\n- Embedding concatenation (model.rs:328) - verify pre_embed, audio_features, post_embed have same hidden_size\n- Token input preparation (model.rs:390) - verify next_input shape\n- Logits extraction (model.rs:355) - verify 3D shape before indexing\n\n**Benefits:**\n- Catch shape mismatches before they cause cryptic errors\n- Easier debugging\n- Better error messages\n\n**Location:** src/model.rs","status":"open","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:31.722775716-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T13:24:31.722775716-03:00"}
{"id":"mutranscriber-5dp","title":"Investigate task-specific tokens for ASR","description":"Whisper uses task-specific tokens like \u003c|transcribe|\u003e, \u003c|translate|\u003e, \u003c|notimestamps|\u003e. Qwen3-ASR may have similar tokens that need to be included in the prompt.\n\n**Investigation needed:**\n1. Check tokenizer vocabulary for task tokens (transcribe, asr, speech, etc.)\n2. Look for \u003c|asr_text|\u003e token ID (mentioned in paper output format)\n3. Check if there are timestamp-related tokens\n4. Review Qwen3-ASR toolkit source for prompt construction\n\n**Potentially relevant token IDs:**\n- 151643: \u003c|endoftext|\u003e\n- 151644: \u003c|im_start|\u003e\n- 151645: \u003c|im_end|\u003e\n- 151669: \u003c|audio_start|\u003e\n- 151670: \u003c|audio_end|\u003e\n- 151676: audio placeholder\n- ???: \u003c|asr_text|\u003e (from paper, ID unknown)\n\n**Location:** src/model.rs, vocab.json from HuggingFace","status":"open","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.514037461-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:24:23.514037461-03:00"}
{"id":"mutranscriber-ajh","title":"Investigate transcription accuracy issues","description":"The transcription pipeline produces coherent output but with word accuracy issues.\n\n**Expected:** 'please visit LibriVox.org. Recording by Moira Fogarty. The Art of War by Sun Tzu. Translated by Lionel'\n\n**Actual:** 'visit leopoldoggs.org. Recording by Mario Fogarty. The Art of Borg by Sonzo. Translated by Lionel.'\n\n**Issues:**\n- Missing 'please' at start\n- 'LibriVox' → 'leopoldoggs'\n- 'Moira' → 'Mario'\n- 'The Art of War' → 'The Art of Borg'\n- 'Sun Tzu' → 'Sonzo'\n\n**Potential causes to investigate:**\n1. Prompt format - may not match what model expects\n2. Audio encoding quality - mel spectrogram or encoder issues\n3. Model configuration - check against reference implementation\n4. Temperature/sampling - currently using greedy, may need different params\n\n**Note:** BPE decoding, KV cache, and logits handling verified correct.","notes":"## Investigation Summary\n\n### Verified Correct:\n- Mel spectrogram parameters (n_fft=400, hop_length=160, n_mels=128, sample_rate=16000) match WhisperFeatureExtractor config\n- Whisper-style log-mel normalization (clamp to max-8.0, then (x+4.0)/4.0)\n- Audio encoder architecture (Conv2D 8x downsampling + Transformer + output projection)\n- KV cache and position tracking\n- Logits extraction and token generation\n- BPE decoding with ByteLevel decoder\n\n### Current Output:\n- Coherent sentences with correct structure\n- Word-level accuracy issues (LibriVox→leapfrogdocs, Moira→Moore, War→Frog)\n- No truncation or repetition problems\n\n### Potential Remaining Causes:\n1. Model inherent limitations (0.6B is smaller model)\n2. Prompt format - model may expect specific task tokens\n3. Subtle audio preprocessing differences vs reference implementation\n4. Missing language specification in prompt\n\n### Next Steps to Try:\n1. Test with 1.7B model to see if accuracy improves\n2. Compare audio encoder output statistics with reference implementation\n3. Try explicit language hint in prompt\n4. Investigate if model needs specific task tokens like Whisper uses","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:14:48.642431319-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:24:28.63139334-03:00","closed_at":"2026-02-02T14:24:28.63139334-03:00","close_reason":"Broken down into specific sub-tasks: mutranscriber-d36, mutranscriber-2lu, mutranscriber-sp4, mutranscriber-5dp"}
{"id":"mutranscriber-b84","title":"Fix logits shape handling in text generation","description":"Logits tensor dimension handling is fragile and may cause incorrect token selection.\n\n**Symptom:** Nonsense output, early stopping, wrong tokens selected\n\n**Root cause:** model.rs:354-360\n- forward_embeds() returns (batch=1, seq=1, vocab_size) - only last token logits\n- Subsequent forward() calls may have different shapes\n- No validation that tensor is 3D (batch, seq, vocab)\n- Indexing with logits.dim(1)? - 1 assumes specific shape\n\n**Fix needed:**\n- Validate logits tensor shape after each forward call\n- Ensure consistent handling between forward_embeds and forward\n- Add assertions for expected dimensions\n\n**Location:** src/model.rs lines 354-360, 391","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:18.426884629-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:14:40.078802398-03:00","closed_at":"2026-02-02T14:14:40.078802398-03:00","close_reason":"Verified implementation is correct - issues are transcription accuracy, not generation bugs"}
{"id":"mutranscriber-d36","title":"Test transcription accuracy with 1.7B model","description":"Test if the larger Qwen3-ASR-1.7B model produces better transcription accuracy than the 0.6B model.\n\n**Current 0.6B output:**\n'visit leapfrogdocs.org. Recording by Moore Froggy. The art of Frog by Sonzo. Translated by Lionel.'\n\n**Expected:**\n'please visit LibriVox.org. Recording by Moira Fogarty. The Art of War by Sun Tzu. Translated by Lionel'\n\n**Test approach:**\n1. Run integration test with --features large or model selection\n2. Compare word error rate between models\n3. Document if accuracy issues are model-size related or implementation-related\n\n**Location:** tests/integration_test.rs, src/model.rs","notes":"## Investigation Results\n\n**Config fixes applied:**\n- rope_theta: 10000 → 1000000 (correct for Qwen3)\n- max_position_embeddings: 4096 → 65536 (correct)\n- Added system message to prompt template\n\n**Finding:** Both 0.6B and 1.7B models produce subpar accuracy. 1.7B is actually worse, indicating implementation issue not model quality.\n\n**Current 0.6B output:** 'visit leapfrog.org. Recording by Morro Froggy, leapfrog.org by Sun Zhu, translated by Lionel.'\n**Expected:** 'please visit LibriVox.org. Recording by Moira Fogarty. The Art of War by Sun Tzu. Translated by Lionel'\n\n**Positives:**\n- 'Sun Zhu' is close to 'Sun Tzu'\n- 'translated by Lionel' is correct\n- Coherent sentence structure\n\n**Remaining issues likely in:**\n- Mel spectrogram extraction (subtle normalization differences)\n- Audio feature alignment with text embeddings\n- Need to compare with reference Python implementation","status":"closed","priority":1,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.164466936-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T15:12:45.671019925-03:00","closed_at":"2026-02-02T15:12:45.671019925-03:00","close_reason":"Tested both models, found and fixed config issues (rope_theta, max_position_embeddings, system message). Accuracy still subpar - needs deeper investigation with reference impl."}
{"id":"mutranscriber-dku","title":"Improve special token handling logic","description":"The special token threshold logic is arbitrary and undocumented.\n\n**Current behavior:** model.rs:378-385\n- Tokens \u003e= 151643 are skipped but still fed to decoder\n- No documentation why 151643 is the threshold\n- May incorrectly skip needed tokens or preserve unwanted ones\n\n**Fix needed:**\n- Document the special token range (151643-151935 based on vocab_size 151936)\n- Use tokenizer's special token detection instead of hardcoded threshold\n- Consider if skipped tokens should be fed to decoder at all\n- Add tests for special token handling\n\n**Location:** src/model.rs lines 377-385","status":"open","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:32.090953378-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T13:24:32.090953378-03:00"}
{"id":"mutranscriber-jb9","title":"Add configurable generation parameters","description":"Currently generation parameters are hardcoded. Add a GenerationConfig struct for better control and debugging.\n\n**Current hardcoded values:**\n- max_new_tokens = 256 (model.rs:337)\n- Greedy decoding only (no temperature/top-k/top-p)\n- EOS token with fallback to hardcoded 151643\n\n**Proposed GenerationConfig:**\n- max_new_tokens: usize\n- temperature: Option\u003cf32\u003e\n- top_k: Option\u003cusize\u003e\n- top_p: Option\u003cf32\u003e\n- eos_token_id: u32\n- repetition_penalty: Option\u003cf32\u003e\n\n**Benefits:**\n- Runtime control without recompilation\n- Better debugging capability\n- Support for different generation strategies\n\n**Location:** src/model.rs, src/config.rs","status":"open","priority":2,"issue_type":"feature","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:31.883644731-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T13:24:31.883644731-03:00"}
{"id":"mutranscriber-sox","title":"Fix BPE subword token decoding - words split incorrectly","description":"The tokenizer decode process doesn't properly handle GPT-2 BPE subword joining. Tokens without Ġ prefix are subword continuations and should be joined without spaces, but currently each token becomes a separate word.\n\n**Symptom:** 'Foggy' outputs as 'F og gy', 'LibriVox' as 'Libre F og gy'\n\n**Root cause:** model.rs:401-423 only replaces Ġ with space but doesn't concatenate subword tokens.\n\n**Fix needed:**\n- Identify word-starting tokens (with Ġ prefix)\n- Join consecutive subword tokens (no prefix) without spaces\n- Handle byte-level fallback tokens\n- Normalize unusual Unicode characters\n\n**Location:** src/model.rs lines 401-423","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:17.915008435-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T13:47:54.433455206-03:00","closed_at":"2026-02-02T13:47:54.433455206-03:00","close_reason":"Fixed by adding ByteLevel decoder to tokenizer"}
{"id":"mutranscriber-sp4","title":"Try explicit language hint in transcription prompt","description":"The model may perform better with explicit language specification in the prompt.\n\n**Current prompt format:**\n`\u003c|im_start|\u003euser\\n\u003c|audio_start|\u003e[audio]\u003c|audio_end|\u003e\u003c|im_end|\u003e\\n\u003c|im_start|\u003eassistant\\n`\n\n**Try these variations:**\n1. Add 'Transcribe in English:' instruction\n2. Add system message with language context\n3. Try 'language English' prefix in user message\n4. Check if model has language-specific tokens (like Whisper's \u003c|en|\u003e)\n\n**Expected output format (from paper):**\n`\u003c|im_start|\u003eassistant language English\u003c|asr_text|\u003etranscription\u003c|im_end|\u003e`\n\nThe model outputs 'language X' prefix but we're not seeing it - may indicate prompt format issue.\n\n**Location:** src/model.rs generate() function","status":"open","priority":2,"issue_type":"task","owner":"me@cfcosta.com","created_at":"2026-02-02T14:24:23.403584912-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:24:23.403584912-03:00"}
{"id":"mutranscriber-xyg","title":"Fix KV cache position tracking in generation loop","description":"The autoregressive generation loop has broken position tracking that corrupts the KV cache, causing missing content and incomplete sentences.\n\n**Symptom:** Transcription cuts off early, missing words, incomplete sentences\n\n**Root cause:** model.rs:351-392\n- position starts at total_prompt_len but KV cache was built with offset=0\n- forward_embeds() processes full sequence, then forward() is called iteratively\n- Position offset for RoPE doesn't align with KV cache state\n- KV cache isn't properly extended for newly generated tokens\n\n**Fix needed:**\n- Clear and rebuild KV cache at generation start OR\n- Properly track sequence position for RoPE relative to KV cache\n- Ensure each new token is processed with correct context\n\n**Location:** src/model.rs lines 351-392, src/qwen3_decoder.rs","status":"closed","priority":0,"issue_type":"bug","owner":"me@cfcosta.com","created_at":"2026-02-02T13:24:18.139069045-03:00","created_by":"Cainã Costa","updated_at":"2026-02-02T14:14:40.077226988-03:00","closed_at":"2026-02-02T14:14:40.077226988-03:00","close_reason":"Verified implementation is correct - issues are transcription accuracy, not generation bugs"}
